import pyzed.sl as sl
import mediapipe as mp
import cv2 as cv
import numpy as np



zed = sl.Camera()

mp_drawing = mp.solutions.drawing_utils
mp_hands = mp.solutions.hands

frame_shape = [720, 1280]

# create a camera configuration object
init_params = sl.InitParameters()
init_params.camera_resolution = sl.RESOLUTION.HD720  # set camera resolution
init_params.camera_fps = 30  # set camera frame rate

#create hand keypoints detector object.
hands0 = mp_hands.Hands(min_detection_confidence=0.5, max_num_hands =1, min_tracking_confidence=0.5)
hands1 = mp_hands.Hands(min_detection_confidence=0.5, max_num_hands =1, min_tracking_confidence=0.5)

# open the camera
err = zed.open(init_params)
if err != sl.ERROR_CODE.SUCCESS:
    print("Failed to open the camera")
    exit()

# set camera intrinsics and extrinsics
calibration_params = zed.get_camera_information().calibration_parameters
P0 = calibration_params.left_cam.projection  # projection matrix for left camera
P1 = calibration_params.right_cam.projection  # projection matrix for right camera





kpts_cam0 = []
kpts_cam1 = []
kpts_3d = []

runtime_params = sl.RuntimeParameters()
while True:
    # retrieve stereo images and depth map from the camera
    if zed.grab(runtime_params) == sl.ERROR_CODE.SUCCESS:
        # retrieve the left and right images
        left_image = sl.Mat()
        right_image = sl.Mat()
        zed.retrieve_image(left_image, sl.VIEW.LEFT)
        zed.retrieve_image(right_image, sl.VIEW.RIGHT)

        # retrieve the depth map
        depth = sl.Mat()
        zed.retrieve_measure(depth, sl.MEASURE.DEPTH)

        # convert the images to OpenCV format
        frame0 = left_image.get_data()
        frame1 = right_image.get_data()

        # crop the frames to 720x720
        frame0 = frame0[:, frame_shape[1] // 2 - frame_shape[0] // 2:frame_shape[1] // 2 + frame_shape[0] // 2]
        frame1 = frame1[:, frame_shape[1] // 2 - frame_shape[0] // 2:frame_shape[1] // 2 + frame_shape[0] // 2]

        # convert the BGR images to RGB
        frame0 = cv.cvtColor(frame0, cv.COLOR_BGR2RGB)
        frame1 = cv.cvtColor(frame1, cv.COLOR_BGR2RGB)

        # detect hand keypoints using MediaPipe
        results0 = hands0.process(frame0)
        results1 = hands1.process(frame1)

        # prepare list of hand keypoints of this frame
        # frame0 kpts
        frame0_keypoints = []
        if results0.multi_hand_landmarks:
            for hand_landmarks in results0.multi_hand_landmarks:
                for p in range(21):
                    # print(p, ':', hand_landmarks.landmark[p].x, hand_landmarks.landmark[p].y)
                    pxl_x = int(round(frame0.shape[1] * hand_landmarks.landmark[p].x))
                    pxl_y = int(round(frame0.shape[0] * hand_landmarks.landmark[p].y))
                kpts = [pxl_x, pxl_y]
                frame0_keypoints.append(kpts)
    else:
        # if no keypoints are found, simply fill the frame data with [-1,-1] for each kpt
        frame0_keypoints = [[-1, -1]] * 21

    # update keypoints container
    kpts_cam0.append(frame0_keypoints)

    # frame1 kpts
    frame1_keypoints = []
    if results1.multi_hand_landmarks:
        for hand_landmarks in results1.multi_hand_landmarks:
            for p in range(21):
                # print(p, ':', hand_landmarks.landmark[p].x, hand_landmarks.landmark[p].y)
                pxl_x = int(round(frame1.shape[1] * hand_landmarks.landmark[p].x))
                pxl_y = int(round(frame1.shape[0] * hand_landmarks.landmark[p].y))
                kpts = [pxl_x, pxl_y]
                frame1_keypoints.append(kpts)
    else:
        # if no keypoints are found, simply fill the frame data with [-1,-1] for each kpt
        frame1_keypoints = [[-1, -1]] * 21

    # update keypoints container
    kpts_cam1.append(frame1_keypoints)

    # calculate 3d position using depth map
    frame_p3ds = []
    for uv1, uv2 in zip(frame0_keypoints, frame1_keypoints):
        if uv1[0] == -1 or uv2[0] == -1:
            _p3d = [-1, -1, -1]
        else:
            x, y = uv1
            depth_value = depth.get_value(y, x) / 1000.0  # divide by 1000 to convert to meters
            _p3d = [x * depth_value, y * depth_value, depth_value]
        frame_p3ds.append(_p3d)

    # this contains the 3d position of each keypoint in current frame.
    # for real time application, this is what you want.
    frame_p3ds = np.array(frame_p3ds).reshape((21, 3))
    kpts_3d.append(frame_p3ds)

    # draw the hand annotations on the image
    frame0 = cv.cvtColor(frame0, cv.COLOR_RGB2BGR)
    frame1 = cv.cvtColor(frame1, cv.COLOR_RGB2BGR)

    if results0.multi_hand_landmarks:
        for hand_landmarks in results0.multi_hand_landmarks:
            mp_drawing.draw_landmarks(frame0, hand_landmarks, mp_hands.HAND_CONNECTIONS)

    if results1.multi_hand_landmarks:
        for hand_landmarks in results1.multi_hand_landmarks:
            mp_drawing.draw_landmarks(frame1, hand_landmarks, mp_hands.HAND_CONNECTIONS)

    cv.imshow('cam1', frame1)
    cv.imshow('cam0', frame0)

    k = cv.waitKey(1)
    if k & 0xFF == 27:
        break  # 27 is ESC key.


    zed.close()

# destroy the windows
cv.destroyAllWindows()